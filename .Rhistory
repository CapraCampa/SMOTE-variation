p1 <- ggplot(trainset, aes(x = X1, y = X2, color = factor(y))) +
geom_point(aes(size = factor(y)), alpha = 0.8, show.legend = c(color = TRUE, size = FALSE)) +
scale_color_manual(values = c("grey", "blue")) +
scale_size_manual(values = c(1, 2)) +
geom_point(data = syn.data.smote, aes(x = X1, y = X2, color = factor(class)), shape = 17, alpha = 0.4) +
labs(title = "SMOTE", x = "Feature 1", y = "Feature 2", color = "Class") +
theme_minimal()
#print(p1)
data.smote$class <- factor(data.smote$class)
# balance dataset with SMOTE variant
smote.dirichlet <- SMOTE.DIRICHLET(trainset[,1:2], trainset$y, K = K, dup_size = 0)
data.smote.dirichlet <- smote.dirichlet$data
syn.data.smote.dirichlet <- smote.dirichlet$syn_data
x <- data.smote.dirichlet[,1:2]
y <- data.smote.dirichlet$class
p2 <- ggplot(x, aes(x = X1, y = X2, color = factor(y))) +
geom_point(aes(size = factor(y)), alpha = 0.8, show.legend = c(color = TRUE, size = FALSE)) +
scale_color_manual(values = c("grey", "blue")) +
scale_size_manual(values = c(1, 2)) +
geom_point(data = syn.data.smote.dirichlet, aes(x = X1, y = X2, color = factor(class)), shape = 17, alpha = 0.4) +
labs(title = "SMOTE.DIRICHLET", x = "Feature 1", y = "Feature 2", color = "Class") +
theme_minimal()
#print(p2)
# Combine the two plots side-by-side
combined_plot <- p1 + p2 +
plot_layout(ncol = 2) # Arrange plots in a single row
#print(combined_plot)
data.smote.dirichlet$class <- factor(data.smote.dirichlet$class)
# apply models (tree and logistic regression) to all train sets -----------
# Classification trees
# model trained on original unbalanced data
tree <- rpart(y ~ ., data = trainset)
best.cp <- tree$cptable[which.min(tree$cptable[,"xerror"]),]
tree <- prune(tree, cp = best.cp[1])
# model trained on data balanced using smote
tree.smote <- rpart(class ~ ., data = data.smote)
best.cp <- tree.smote$cptable[which.min(tree.smote$cptable[,"xerror"]),]
tree.smote <- prune(tree.smote, cp = best.cp[1])
# model trained on data balanced using smote dirichlet
tree.smote.dirichlet <- rpart(class ~ ., data = data.smote.dirichlet)
best.cp <- tree.smote.dirichlet$cptable[which.min(tree.smote.dirichlet$cptable[,"xerror"]),]
tree.smote.dirichlet <- prune(tree.smote.dirichlet, cp = best.cp[1])
# Logistic regression
fit <- glm(y ~ . , data = trainset, family = binomial(link = "logit"))
fit.smote <- glm(class ~ . , data = data.smote, family = binomial(link = "logit"))
fit.smote.dirichlet <- glm(class ~ . , data = data.smote.dirichlet, family = binomial(link = "logit"))
# Find and plot test set ---------------------------------------------
test_index <- ceiling((i/3))
testset <- testsets[[test_index]]
testset$y <- factor(testset$y, levels = c(0, 1))
p_test <- ggplot(testset, aes(x = X1, y = X2, color = factor(y))) +
geom_point(aes(size = factor(y)), alpha = 0.8, show.legend = c(color = TRUE, size = FALSE)) +
scale_color_manual(values = c("grey", "blue")) +
scale_size_manual(values = c(1, 2)) +
labs(title = "SMOTE", x = "Feature 1", y = "Feature 2", color = "Class") +
theme_minimal()
#print(p_test)
###########################################################################
validationset <- validationsets[[test_index]]
validationset$y <- factor(validationset$y, levels=c(0,1))
pred.tree <- predict(tree, newdata = validationset,type = "prob")
pred.tree.smote <- predict(tree.smote, newdata = validationset,type = "prob")
pred.tree.smote.dirichlet <- predict(tree.smote.dirichlet, newdata = validationset)
threshold = find_optimal_threshold(pred.tree[,2],validationset$y)
threshold.tree.smote <- find_optimal_threshold(pred.tree.smote[,2],validationset$y)
threshold.tree.smote.dirichlet <- find_optimal_threshold(pred.tree.smote.dirichlet[,2],validationset$y)
###############
pred.tree <- predict(tree, newdata = testset,type = "prob")
pred.tree.smote <- predict(tree.smote, newdata = testset,type = "prob")
pred.tree.smote.dirichlet <- predict(tree.smote.dirichlet, newdata = testset)
pred.tree.class <- ifelse(pred.tree[,2] > threshold, 1, 0)
pred.tree.smote.class <- ifelse(pred.tree.smote[,2] > threshold.tree.smote, 1, 0)
pred.tree.smote.dirichlet.class <- ifelse(pred.tree.smote.dirichlet[,2] > threshold.tree.smote.dirichlet, 1, 0)
#################
prob_class_1 <- predict(fit, newdata = validationset, type= "response")
prob_class_0 <- 1 - prob_class_1
pred.fit <- cbind(prob_class_0, prob_class_1)
prob_class_1 <- predict(fit.smote, newdata = validationset, type="response")
prob_class_0 <- 1 - prob_class_1
pred.fit.smote <- cbind(prob_class_0, prob_class_1)
prob_class_1 <- predict(fit.smote.dirichlet, newdata = validationset, type="response")
prob_class_0 <- 1 - prob_class_1
pred.fit.smote.dirichlet <- cbind(prob_class_0, prob_class_1)
threshold <- find_optimal_threshold(pred.fit[,2],validationset$y)
threshold.fit.smote <- find_optimal_threshold(pred.fit.smote[,2],validationset$y)
threshold.fit.smote.dirichlet <- find_optimal_threshold(pred.fit.smote.dirichlet[,2],validationset$y)
################
prob_class_1 <- predict(fit, newdata = testset, type= "response")
prob_class_0 <- 1 - prob_class_1
pred.fit <- cbind(prob_class_0, prob_class_1)
prob_class_1 <- predict(fit.smote, newdata = testset, type="response")
prob_class_0 <- 1 - prob_class_1
pred.fit.smote <- cbind(prob_class_0, prob_class_1)
prob_class_1 <- predict(fit.smote.dirichlet, newdata = testset, type="response")
prob_class_0 <- 1 - prob_class_1
pred.fit.smote.dirichlet <- cbind(prob_class_0, prob_class_1)
pred.fit.class <- ifelse(pred.fit[,2] > threshold, 1, 0)
pred.fit.smote.class <- ifelse(pred.fit.smote[,2] > threshold.fit.smote, 1, 0)
pred.fit.smote.dirichlet.class <- ifelse(pred.fit.smote.dirichlet[,2] > threshold.fit.smote.dirichlet, 1, 0)
pred.fit.class <- factor(pred.fit.class, levels = c(0,1))
pred.fit.smote.class <- factor(pred.fit.smote.class, levels = c(0,1))
pred.fit.smote.dirichlet.class <- factor(pred.fit.smote.dirichlet.class,levels = c(0,1))
#############
# Find optimal threshold ------------------------
# pred.tree <- predict(tree, newdata = testset,type = "prob")
# pred.tree.smote <- predict(tree.smote, newdata = testset,type = "prob")
# pred.tree.smote.dirichlet <- predict(tree.smote.dirichlet, newdata = testset)
#
# #threshold = 0.5
# #threshold.tree.smote = 0.5
# #threshold.tree.smote.dirichlet = 0.5
# threshold.tree.smote <- find_optimal_threshold(pred.tree.smote[,2],testset$y)
# threshold = find_optimal_threshold(pred.tree[,2],testset$y)
# threshold.tree.smote.dirichlet <- find_optimal_threshold(pred.tree.smote.dirichlet[,2],testset$y)
#
# #cat(trainset_name,"\n")
# #cat("Tree thresholds: ", threshold, threshold.tree.smote, threshold.tree.smote.dirichlet, "\n")
# ###############
# pred.tree.class <- ifelse(pred.tree[,2] > threshold, 1, 0)
# pred.tree.smote.class <- ifelse(pred.tree.smote[,2] > threshold.tree.smote, 1, 0)
# pred.tree.smote.dirichlet.class <- ifelse(pred.tree.smote.dirichlet[,2] > threshold.tree.smote.dirichlet, 1, 0)
# #################
#
# prob_class_1 <- predict(fit, newdata = testset, type= "response")
# prob_class_0 <- 1 - prob_class_1
# pred.fit <- cbind(prob_class_0, prob_class_1)
#
# prob_class_1 <- predict(fit.smote, newdata = testset, type="response")
# prob_class_0 <- 1 - prob_class_1
# pred.fit.smote <- cbind(prob_class_0, prob_class_1)
#
# prob_class_1 <- predict(fit.smote.dirichlet, newdata = testset, type="response")
# prob_class_0 <- 1 - prob_class_1
# pred.fit.smote.dirichlet <- cbind(prob_class_0, prob_class_1)
#
# #threshold= 0.2
# #threshold.fit.smote <- 0.75
# #threshold.fit.smote.dirichlet <- 0.75
# threshold <- find_optimal_threshold(pred.fit[,2],testset$y)
# threshold.fit.smote <- find_optimal_threshold(pred.fit.smote[,2],testset$y)
# threshold.fit.smote.dirichlet <- find_optimal_threshold(pred.fit.smote.dirichlet[,2],testset$y)
#
# #cat("Fit thresholds: ", threshold, threshold.fit.smote, threshold.fit.smote.dirichlet, "\n")
# ################
# pred.fit.class <- ifelse(pred.fit[,2] > threshold, 1, 0)
# pred.fit.smote.class <- ifelse(pred.fit.smote[,2] > threshold.fit.smote, 1, 0)
# pred.fit.smote.dirichlet.class <- ifelse(pred.fit.smote.dirichlet[,2] > threshold.fit.smote.dirichlet, 1, 0)
#
# pred.fit.class <- factor(pred.fit.class, levels = c(0,1))
# pred.fit.smote.class <- factor(pred.fit.smote.class, levels = c(0,1))
# pred.fit.smote.dirichlet.class <- factor(pred.fit.smote.dirichlet.class,levels = c(0,1))
# #############
#
# compute metrics to compare our variant to the original technique --------
# Logistic regression
metrics.fit <- accuracy.meas(response = testset$y, predicted = pred.fit[,2], threshold = threshold)
metrics.fit.smote <- accuracy.meas(response = testset$y, predicted = pred.fit.smote[,2], threshold = threshold.fit.smote)
metrics.fit.smote.dirichlet <- accuracy.meas(response = testset$y, predicted = pred.fit.smote.dirichlet[,2], threshold = threshold.fit.smote.dirichlet)
auc.fit <- roc.curve(testset$y, pred.fit[,2], plotit=FALSE,col = "darkgreen", main = paste("ROC Curve - Dataset:", trainset_name, "\nLog regression"))$auc
auc.fit.smote <- roc.curve(testset$y, pred.fit.smote[,2], plotit=FALSE,add.roc = TRUE, col = "orange")$auc
auc.fit.smote.dirichlet <- roc.curve(testset$y, pred.fit.smote.dirichlet[,2], add.roc = TRUE,plotit=FALSE, col = "purple")$auc
##############
cm <- confusionMatrix(pred.fit.class, testset$y, positive = "1")
acc.fit <- cm$byClass["Balanced Accuracy"]
cm.smote <- confusionMatrix(pred.fit.smote.class, testset$y, positive = "1")
acc.fit.smote <- cm.smote$byClass["Balanced Accuracy"]
cm.smote.dirichlet <- confusionMatrix(pred.fit.smote.dirichlet.class, testset$y, positive = "1")
acc.fit.smote.dirichlet <- cm.smote.dirichlet$byClass["Balanced Accuracy"]
#############
results[[i]][["logistic_regressor"]][[1]]$auc[k] <- auc.fit
if (is.nan(metrics.fit$precision)| is.na(metrics.fit$precision)){
results[[i]][["logistic_regressor"]][[1]]$precision[k] <- 0
}else{
results[[i]][["logistic_regressor"]][[1]]$precision[k] <- metrics.fit$precision
}
results[[i]][["logistic_regressor"]][[1]]$recall[k] <- metrics.fit$recall
if (is.nan(metrics.fit$F)| is.na(metrics.fit$F)){
results[[i]][["logistic_regressor"]][[1]]$f1[k] <- 0
}else{
results[[i]][["logistic_regressor"]][[1]]$f1[k] <- metrics.fit$F
}
####
results[[i]][["logistic_regressor"]][[1]]$balanced_acc[k] <- acc.fit
####
results[[i]][["logistic_regressor"]][[2]]$auc[k] <- auc.fit.smote
results[[i]][["logistic_regressor"]][[2]]$precision[k] <- metrics.fit.smote$precision
results[[i]][["logistic_regressor"]][[2]]$recall[k] <- metrics.fit.smote$recall
if (is.nan(metrics.fit.smote$F)| is.na(metrics.fit.smote$F)){
results[[i]][["logistic_regressor"]][[2]]$f1[k] <- 0
}else{
results[[i]][["logistic_regressor"]][[2]]$f1[k] <- metrics.fit.smote$F
}
####
results[[i]][["logistic_regressor"]][[2]]$balanced_acc[k] <- acc.fit.smote
####
results[[i]][["logistic_regressor"]][[3]]$auc[k] <- auc.fit.smote.dirichlet
results[[i]][["logistic_regressor"]][[3]]$precision[k] <- metrics.fit.smote.dirichlet$precision
results[[i]][["logistic_regressor"]][[3]]$recall[k] <- metrics.fit.smote.dirichlet$recall
if (is.nan(metrics.fit.smote.dirichlet$F)| is.na(metrics.fit.smote.dirichlet$F)){
results[[i]][["logistic_regressor"]][[3]]$f1[k] <- 0
}else{
results[[i]][["logistic_regressor"]][[3]]$f1[k] <- metrics.fit.smote.dirichlet$F
}
####
results[[i]][["logistic_regressor"]][[3]]$balanced_acc[k] <- acc.fit.smote.dirichlet
####
# Classification trees
metrics.tree <- accuracy.meas(response = testset$y, predicted = pred.tree[,2], threshold = threshold)
metrics.tree.smote <- accuracy.meas(response = testset$y, predicted = pred.tree.smote[,2], threshold = threshold.fit.smote)
metrics.tree.smote.dirichlet <- accuracy.meas(response = testset$y, predicted = pred.tree.smote.dirichlet[,2], threshold = threshold.fit.smote.dirichlet)
auc.tree <- roc.curve(testset$y, pred.tree[,2], main = paste("ROC Curve - Dataset:", trainset_name, "\nTree"), plotit=FALSE)$auc
auc.tree.smote <- roc.curve(testset$y, pred.tree.smote[,2],add.roc = TRUE, plotit=FALSE, col = 2)$auc
auc.tree.smote.dirichlet <- roc.curve(testset$y, pred.tree.smote.dirichlet[,2], add.roc = TRUE, plotit=FALSE, col = 3)$auc
########
cm <- confusionMatrix(factor(pred.tree.class, levels=c(0,1)), testset$y, positive = "1")
acc.tree <- cm$byClass["Balanced Accuracy"]
cm <- confusionMatrix(factor(pred.tree.smote.class, levels = c(0,1)), testset$y, positive = "1")
acc.tree.smote <- cm$byClass["Balanced Accuracy"]
cm <- confusionMatrix(factor(pred.tree.smote.dirichlet.class, levels=c(0,1)), testset$y, positive = "1")
acc.tree.smote.dirichlet <- cm$byClass["Balanced Accuracy"]
########
results[[i]][["decision_tree"]][[1]]$auc[k] <- auc.tree
if (is.nan(metrics.tree$precision) | is.na(metrics.tree$precision)){
results[[i]][["decision_tree"]][[1]]$precision[k] <- 0
}else{
results[[i]][["decision_tree"]][[1]]$precision[k] <- metrics.tree$precision
}
results[[i]][["decision_tree"]][[1]]$recall[k] <- metrics.tree$recall
if (is.nan(metrics.tree$F)| is.na(metrics.tree$F)){
results[[i]][["decision_tree"]][[1]]$f1[k] <- 0
}else{
results[[i]][["decision_tree"]][[1]]$f1[k] <- metrics.tree$F
}
####
results[[i]][["decision_tree"]][[1]]$balanced_acc[k] <- acc.tree
####
results[[i]][["decision_tree"]][[2]]$auc[k] <- auc.tree.smote
results[[i]][["decision_tree"]][[2]]$precision[k] <- metrics.tree.smote$precision
results[[i]][["decision_tree"]][[2]]$recall[k] <- metrics.tree.smote$recall
if (is.nan(metrics.tree.smote$F)| is.na(metrics.tree.smote$F)){
results[[i]][["decision_tree"]][[2]]$f1[k] <- 0
}else{
results[[i]][["decision_tree"]][[2]]$f1[k] <- metrics.tree.smote$F
}
####
results[[i]][["decision_tree"]][[2]]$balanced_acc[k] <- acc.tree.smote
####
results[[i]][["decision_tree"]][[3]]$auc[k] <- auc.tree.smote.dirichlet
results[[i]][["decision_tree"]][[3]]$precision[k] <- metrics.tree.smote.dirichlet$precision
results[[i]][["decision_tree"]][[3]]$recall[k] <- metrics.tree.smote.dirichlet$recall
if (is.nan(metrics.tree.smote.dirichlet$F)| is.na(metrics.tree.smote.dirichlet$F)){
results[[i]][["decision_tree"]][[3]]$f1[k] <- 0
}else{
results[[i]][["decision_tree"]][[3]]$f1[k] <- metrics.tree.smote.dirichlet$F
}
####
results[[i]][["decision_tree"]][[3]]$balanced_acc[k] <- acc.tree.smote.dirichlet
####
}
}
# Boxplot of AUC under Decision Tree --------------------------------------
# Specify the desired order of levels for trainset
levels <- c(
"train_600_001", "train_600_0025", "train_600_005", "train_600_01",
"train_1000_001", "train_1000_0025", "train_1000_005", "train_1000_01",
"train_5000_001", "train_5000_0025", "train_5000_005", "train_5000_01"
)
# Prepare data for plotting
plot_data <- data.frame(
trainset = integer(),
version = integer(),
auc = numeric()
)
for (l in 1:9) {
for (version in 1:3) {
auc_values <- results[[l]]$decision_tree[[version]]$auc
temp_df <- data.frame(
trainset =  names(trainsets)[l],
version = version,
auc = auc_values
)
plot_data <- rbind(plot_data, temp_df)
}
}
# Convert trainset to a factor with the specified levels
plot_data$trainset <- factor(plot_data$trainset, levels = levels)
# Create the plot
auc_dt <- ggplot(plot_data, aes(x = factor(version), y = auc, fill = factor(version))) +
geom_boxplot() +
facet_wrap(~ trainset, ncol = 3) +
scale_fill_manual(
values = c("#1b9e77", "#d95f02", "#7570b3"),
labels = c("Unbalanced data", "SMOTE", "Dirichlet SMOTE")
) +
labs(
title = "Boxplots of AUC values for Decision Tree Model by Version",
x = "Version",
y = "AUC",
fill = "Model Version"  # Legend title
) +
theme_minimal() +
theme(
strip.text = element_text(size = 10, face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
# Boxplot of AUC under Logistic Regression --------------------------------
# Prepare data for plotting
plot_data <- data.frame(
trainset = integer(),
version = integer(),
auc = numeric()
)
for (l in 1:9) {
for (version in 1:3) {
auc_values <- results[[l]]$logistic_regressor[[version]]$auc
temp_df <- data.frame(
trainset =  names(trainsets)[l],
version = version,
auc = auc_values
)
plot_data <- rbind(plot_data, temp_df)
}
}
# Convert trainset to a factor with the specified levels
plot_data$trainset <- factor(plot_data$trainset, levels = levels)
# Create the plot
auc_logistic <- ggplot(plot_data, aes(x = factor(version), y = auc, fill = factor(version))) +
geom_boxplot() +
facet_wrap(~ trainset, ncol = 3) +
scale_fill_manual(
values = c("#1b9e77", "#d95f02", "#7570b3"),
labels = c("Unbalanced data", "SMOTE", "Dirichlet SMOTE")
) +
labs(
title = "Boxplots of AUC values for Logistic Regression Model by Version",
x = "Version",
y = "AUC",
fill = "Model Version"
) +
theme_minimal() +
theme(
strip.text = element_text(size = 10, face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
# -------------------------------------------------------------------------
plot_data <- data.frame(
trainset = integer(),
version = integer(),
f1 = numeric()
)
for (l in 1:9) {
for (version in 1:3) {
f1_values <- results[[l]]$decision_tree[[version]]$f1
temp_df <- data.frame(
trainset =  names(trainsets)[l],
version = version,
f1 = f1_values
)
plot_data <- rbind(plot_data, temp_df)
}
}
# Convert trainset to a factor with the specified levels
plot_data$trainset <- factor(plot_data$trainset, levels = levels)
# Create the plot
f1_dt <- ggplot(plot_data, aes(x = factor(version), y = f1, fill = factor(version))) +
geom_boxplot() +
facet_wrap(~ trainset, ncol = 3) +
scale_fill_manual(
values = c("#1b9e77", "#d95f02", "#7570b3"),
labels = c("Unbalanced data", "SMOTE", "Dirichlet SMOTE")
) +
labs(
title = "Boxplots of F1 values for Decision Tree Model by Version",
x = "Version",
y = "F1",
fill = "Model Version"
) +
theme_minimal() +
theme(
strip.text = element_text(size = 10, face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
plot_data <- data.frame(
trainset = integer(),
version = integer(),
f1 = numeric()
)
for (l in 1:9) {
for (version in 1:3) {
f1_values <- results[[l]]$logistic_regressor[[version]]$f1
temp_df <- data.frame(
trainset =  names(trainsets)[l],
version = version,
f1 = f1_values
)
plot_data <- rbind(plot_data, temp_df)
}
}
# Convert trainset to a factor with the specified levels
plot_data$trainset <- factor(plot_data$trainset, levels = levels)
# Create the plot
f1_logistic <- ggplot(plot_data, aes(x = factor(version), y = f1, fill = factor(version))) +
geom_boxplot() +
facet_wrap(~ trainset, ncol = 3) +
scale_fill_manual(
values = c("#1b9e77", "#d95f02", "#7570b3"),
labels = c("Unbalanced data", "SMOTE", "Dirichlet SMOTE")
) +
labs(
title = "Boxplots of F1 values for Logistic Regression Model by Version",
x = "Version",
y = "F1",
fill = "Model Version"
) +
theme_minimal() +
theme(
strip.text = element_text(size = 10, face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
#--------------------------------------------------------------------------------
plot_data <- data.frame(
trainset = integer(),
version = integer(),
acc = numeric()
)
for (l in 1:9) {
for (version in 1:3) {
acc_values <- results[[l]]$decision_tree[[version]]$balanced_acc
temp_df <- data.frame(
trainset =  names(trainsets)[l],
version = version,
acc = acc_values
)
plot_data <- rbind(plot_data, temp_df)
}
}
# Convert trainset to a factor with the specified levels
plot_data$trainset <- factor(plot_data$trainset, levels = levels)
# Create the plot
acc_dt <- ggplot(plot_data, aes(x = factor(version), y = acc, fill = factor(version))) +
geom_boxplot() +
facet_wrap(~ trainset, ncol = 3) +
scale_fill_manual(
values = c("#1b9e77", "#d95f02", "#7570b3"),
labels = c("Unbalanced data", "SMOTE", "Dirichlet SMOTE")
) +
labs(
title = "Boxplots of balanced accuracy values for Decision Tree Model by Version",
x = "Version",
y = "Balanced Acc",
fill = "Model Version"
) +
theme_minimal() +
theme(
strip.text = element_text(size = 10, face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
plot_data <- data.frame(
trainset = integer(),
version = integer(),
acc = numeric()
)
for (l in 1:9) {
for (version in 1:3) {
acc_values <- results[[l]]$logistic_regressor[[version]]$balanced_acc
temp_df <- data.frame(
trainset =  names(trainsets)[l],
version = version,
acc = acc_values
)
plot_data <- rbind(plot_data, temp_df)
}
}
# Convert trainset to a factor with the specified levels
plot_data$trainset <- factor(plot_data$trainset, levels = levels)
# Create the plot
acc_logistic <- ggplot(plot_data, aes(x = factor(version), y = acc, fill = factor(version))) +
geom_boxplot() +
facet_wrap(~ trainset, ncol = 3) +
scale_fill_manual(
values = c("#1b9e77", "#d95f02", "#7570b3"),
labels = c("Unbalanced data", "SMOTE", "Dirichlet SMOTE")
) +
labs(
title = "Boxplots of balanced accuracy values for Logistic Regression Model by Version",
x = "Version",
y = "acc",
fill = "Model Version"
) +
theme_minimal() +
theme(
strip.text = element_text(size = 10, face = "bold"),
axis.text.x = element_text(angle = 45, hjust = 1)
)
# plotting metrics----------------------------------------------------------------
#combined_metrics <- acc_dt + acc_logistic + f1_dt + f1_logistic + plot_layout(ncol = 2)
#print(combined_metrics)
print((auc_dt +auc_logistic))
}
print(p_test)
testset
test_index
test_index <- 2
testset <- testsets[[test_index]]
testset$y <- factor(testset$y, levels = c(0, 1))
p_test <- ggplot(testset, aes(x = X1, y = X2, color = factor(y))) +
geom_point(aes(size = factor(y)), alpha = 0.8, show.legend = c(color = TRUE, size = FALSE)) +
scale_color_manual(values = c("grey", "blue")) +
scale_size_manual(values = c(1, 2)) +
labs(title = "SMOTE", x = "Feature 1", y = "Feature 2", color = "Class") +
theme_minimal()
print(p_test)
